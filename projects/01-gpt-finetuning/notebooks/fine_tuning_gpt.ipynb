{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I6Vx7xiooDju",
        "outputId": "43615eb4-ced8-4132-eb1e-1dc46544e4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets # Installe la librairie Hugging Face Datasets pour charger des jeux de données standardisés."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -U # Installe ou met à jour Transformers pour télécharger modèles et tokenizers.\n",
        "!pip install accelerate -U # Installe Accelerate pour gérer CPU/GPU/TPU et paralléliser l’entraînement."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3tEtodeAoaXA",
        "outputId": "d60cd346-3cf0-4c45-afd3-e760f737146c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl # Installe TRL (Transformer Reinforcement Learning). Ici utilisé pour l’entraînement supervisé simplifié (SFT)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w9phCi0Aoyqw",
        "outputId": "026d6b66-2a1a-4c01-929e-b456fbcbd52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.11.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Charge PyTorch.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Sélectionne le GPU si présent, sinon le CPU. Servira à placer tenseurs et modèles."
      ],
      "metadata": {
        "id": "HwluwO0Rot0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset # Importe la fonction de chargement HF.\n",
        "DATASET_NAME = \"mlabonne/guanaco-llama2-1k\" # Nom du dataset à récupérer depuis Hugging Face Hub.\n",
        "#Le dataset a un format de type chat/instruction.\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME) # Télécharge et met en cache local le dataset. Retourne un DatasetDict (splits)."
      ],
      "metadata": {
        "collapsed": true,
        "id": "ExvKFTnOpi2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset) # Affiche la structure des splits disponibles."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo1HXvVlp-Yr",
        "outputId": "671ae869-bd97-47b4-c047-1a66c4b52d47",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset['train'] # Sélectionne le split d’entraînement.\n",
        "print(training_dataset) # Affiche nombre d’exemples et les colonnes."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLw_PWwlqCXy",
        "outputId": "55df92ee-cf8c-4d64-9b63-e2d6ad61fc0f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 1000\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset[0] # Indexe le premier exemple. En notebook, affiche un dict."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sa7EvLslr0Jv",
        "outputId": "9fd683b7-389d-4459-89ae-584f476b74ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset[11] # Affiche l’exemple d’index 11."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iACGhj9Ar35B",
        "outputId": "b1ce938e-d82c-4cc7-ce59-7ede6ad126c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '<s>[INST] write me a 1000 words essay about deez nuts. [/INST] The Deez Nuts meme first gained popularity in 2015 on the social media platform Vine. The video featured a young man named Rodney Bullard, who recorded himself asking people if they had heard of a particular rapper. When they responded that they had not, he would respond with the phrase \"Deez Nuts\" and film their reactions. The video quickly went viral, and the phrase became a popular meme. \\n\\nSince then, Deez Nuts has been used in a variety of contexts to interrupt conversations, derail discussions, or simply add humor to a situation. It has been used in internet memes, in popular music, and even in politics. In the 2016 US presidential election, a 15-year-old boy named Brady Olson registered as an independent candidate under the name Deez Nuts. He gained some traction in the polls and even made appearances on national news programs.\\n\\nThe Deez Nuts meme has had a significant impact on popular culture. It has become a recognizable catchphrase that people use to add humor to everyday conversations. The meme has also been used to satirize politics and other serious issues. For example, in 2016, a group of activists in the UK used the phrase \"Deez Nuts for President\" as part of a campaign to encourage young people to vote in the EU referendum. </s><s>[INST] Rewrite the essay in a more casual way. Instead of sounding proffesional, sound like a college student who is forced to write the essay but refuses to do so in the propper way. Use casual words and slang when possible. [/INST] Yo, so you want me to write a 1000-word essay about Deez Nuts? Alright, fine. So, this whole thing started on Vine back in 2015. Some dude named Rodney Bullard made a video where he would ask people if they knew a rapper, and when they said no, he would hit them with the classic line: \"Deez Nuts!\" People loved it, and it became a viral meme.\\n\\nNowadays, Deez Nuts is used for all kinds of stuff. You can throw it out there to interrupt someone or just to be funny. It\\'s all over the internet, in music, and even in politics. In fact, during the 2016 US presidential election, a kid named Brady Olson registered as an independent candidate under the name Deez Nuts. He actually got some attention from the media and made appearances on TV and everything.\\n\\nThe impact of Deez Nuts on our culture is pretty huge. It\\'s become a thing that everyone knows and uses to add some humor to their everyday conversations. Plus, people have used it to make fun of politics and serious issues too. Like, in the UK, some groups of activists used the phrase \"Deez Nuts for President\" to encourage young people to vote in the EU referendum.\\n\\nThere you have it, a thousand words about Deez Nuts in a more casual tone. Can I go back to playing video games now? </s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset[7] # Affiche l’exemple d’index 7.\n",
        "# Objectif de ces trois dernières lignes: inspection rapide du format."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IoX1muZDsGMh",
        "outputId": "75f06d39-fe20-4764-9bd1-88997209f751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': '<s>[INST] Ich möchte für ein Handy-Game den Flug von Papierfliegern simulieren. Was für Formeln können mir helfen um das möglichst realistisch zu tun? [/INST] Um den Flug von Papierfliegern in einem Spiel realistisch zu simulieren, gibt es einige wichtige Formeln, die Sie berücksichtigen sollten:\\n\\nLift-Kraft: Der Lift, der einen Papierflieger in der Luft hält, wird durch die Bewegung der Luft über die Oberfläche des Flügels erzeugt. Die Formel zur Berechnung der Lift-Kraft lautet:\\n\\nL = 0.5 * rho * v^2 * A * Cl\\n\\nL = Lift-Kraft\\nrho = Luftdichte\\nv = Geschwindigkeit des Flugzeugs\\nA = Flügeloberfläche\\nCl = Auftriebsbeiwert des Flügels\\n\\nLuftwiderstand: Der Luftwiderstand ist die Kraft, die gegen den Flug eines Papierfliegers wirkt und durch die Bewegung der Luft entsteht, wenn sie um das Flugzeug herumströmt. Die Formel zur Berechnung des Luftwiderstands lautet:\\n\\nD = 0.5 * rho * v^2 * A * Cd\\n\\nD = Luftwiderstand\\nrho = Luftdichte\\nv = Geschwindigkeit des Flugzeugs\\nA = Querschnittsfläche des Flugzeugs\\nCd = Luftwiderstandsbeiwert des Flugzeugs\\n\\nGravitationskraft: Die Gravitationskraft ist die Kraft, die das Flugzeug zur Erde zieht. Die Formel zur Berechnung der Gravitationskraft lautet:\\n\\nFg = m * g\\n\\nFg = Gravitationskraft\\nm = Masse des Flugzeugs\\ng = Beschleunigung durch die Schwerkraft (9,81 m/s^2)\\n\\nGier, Nick- und Rollwinkel: Um die Flugbewegungen eines Papierfliegers realistisch zu simulieren, müssen Sie auch die Gier-, Nick- und Rollwinkel berücksichtigen. Diese Winkel bestimmen die Ausrichtung des Flugzeugs in der Luft und beeinflussen seine Flugbahn.\\n\\nGierwinkel: Der Gierwinkel beschreibt die Drehung des Flugzeugs um die senkrechte Achse. Es wird normalerweise durch den Ruderausschlag des Seitenruders gesteuert.\\nNickwinkel: Der Nickwinkel beschreibt die Drehung des Flugzeugs um die Querachse. Es wird normalerweise durch den Ruderausschlag des Höhenruders gesteuert.\\nRollwinkel: Der Rollwinkel beschreibt die Drehung des Flugzeugs um die Längsachse. Es wird normalerweise durch den Ruderausschlag des Querruders gesteuert.\\nMit diesen Formeln und den entsprechenden Eingabeparametern (z.B. Flugzeuggröße, Fluggeschwindigkeit, Luftdichte) können Sie den Flug von Papierfliegern in Ihrem Spiel realistisch simulieren. Beachten Sie jedoch, dass auch andere Faktoren wie z.B. Wind und Turbulenzen einen Einfluss auf den Flug haben können, und dass die Simulation möglicherweise aufwendig sein kann. </s><s>[INST] Könntest du mir die Formel bitte als Methoden und C# programmieren, damit ich sie in der Game engine Unity verwenden kann? [/INST] Hier ist eine C#-Klasse, die die Formeln für Lift und Luftwiderstand implementiert. Gravitation ließe sich zwar ebenfalls leicht implementieren, ist in Unity aber bereits eingebaut.\\n\\n```csharp\\nclass PaperPlanePhysics {\\n\\t// Physikalische Konstanten\\n\\tpublic float rho = 1.2041f; // Luftdichte, kg/m³\\n\\t\\n\\t// Parameter des Papierfliegers\\n\\tpublic float A; // Flügeloberfläche, m²\\n\\tpublic float Cl; // Auftriebsbeiwert\\n\\tpublic float Cd; // Luftwiderstandsbeiwert\\n\\t\\n\\t// Auftriebskraft\\n\\tpublic float ComputeLift(float v) {\\n\\t\\treturn 0.5f * rho * v*v * A * Cl;\\n\\t}\\n\\t\\n\\t// Luftwiderstand\\n\\tpublic float ComputeDrag(float v) {\\n\\t\\treturn 0.5f * rho * v*v * A * Cd;\\n\\t}\\n\\t\\n\\t// Resultierende Kräfte als Vektor berechnen\\n\\tpublic Vector3 ComputeForceVector(Vector3 velocity, Vector3 forward, Vector3 up) {\\n\\t\\t// Windgeschwindigkeit glatt entlang des Flügels\\n\\t\\tfloat forwardVelocity = Vector3.Dot(velocity, forward);\\n\\t\\t\\n\\t\\t// Vereinfachung: In der Realität ist die Abhängigkeit vom Winkel zwischen Flügel und Luftgeschwindigkeit komplexer.\\n\\t\\t// Nehmen wir an dass es nur von der Geschwindigkeit gerade nach vorne abhängt.\\n\\t\\tVector3 lift = up * ComputeLift(forwardVelocity);\\n\\t\\t\\n\\t\\t// Vereinfachung: Luftwiderstand wirkt in allen Richtungen, möglicherweise mit unterschiedlichen Faktoren.\\n\\t\\t// Wir berechnen hier nur den Luftwiderstand nach vorne.\\n\\t\\tVector3 drag = -forwardVelocity * ComputeDrag(forwardVelocity);\\n\\t\\t\\n\\t\\treturn lift + drag;\\n\\t}\\n}\\n```\\n\\nIn einer Game-Engine wie Unity benötigst du in der Regel einen 2D- oder 3D-Vektor für die Kräfte. Die Methode `ComputeForceVector` ist ein Beispiel dafür wie du die Vektoren berechnen kannst. </s><s>[INST] Ich arbeite international mit anderen Entwicklern zusammen und schreibe deshalb ausschließlich englischen Code. Kannst du die Kommentare bitte übersetzen und etwas aussagekräftigere Variablennamen für die Flügeloberfläche, den Auftrieb und den Luftwiderstand verwenden? [/INST] Gerne! Hier ist der englische Code mit besseren Variablennamen:\\n\\n```cs\\nclass PaperPlanePhysics {\\n\\n\\t// physical constants\\n\\tpublic float rho = 1.2041f; // air density, kg/m³\\n\\t\\n\\t// characteristics of the paper plane\\n\\tpublic float wingSurf; // wing surface, m²\\n\\tpublic float liftCoeff; // lift coefficient\\n\\tpublic float dragCoeff; // drag coefficient\\n\\t\\n\\t// calculate lift force based on velocity v\\n\\tpublic float ComputeLift(float v) {\\n\\t\\treturn 0.5f * rho * (v * v) * wingSurf * liftCoeff;\\n\\t}\\n\\t\\n\\t// calculate air drag based on velocity v\\n\\tpublic float ComputeDrag(float v) {\\n\\t\\treturn 0.5f * rho * (v * v) * wingSurf * dragCoeff;\\n\\t}\\n\\t\\n\\t// calculate all resulting forces as a vector\\n\\tpublic Vector3 ComputeForceVector(Vector3 velocity, Vector3 forward, Vector3 up) {\\n\\t\\t// wind speed along the wing\\n\\t\\tfloat forwardVelocity = Vector3.Dot(velocity, forward);\\n\\t\\t\\n\\t\\t// Simplification: In reality, the relationship between wing angle and airspeed is much more complex.\\n\\t\\t// We assume that it depends only on the speed straight ahead.\\n\\t\\tVector3 lift = up * ComputeLift(forwardVelocity);\\n\\t\\t\\n\\t\\t// Simplification: Air resistance works in all directions, possibly with different factors.\\n\\t\\t// We only compute the forward drag here.\\n\\t\\tVector3 drag = -forwardVelocity * ComputeDrag(forwardVelocity);\\n\\t\\t\\n\\t\\treturn lift + drag;\\n\\t}\\n}\\n``` </s>'}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"distilgpt2\" # Choisit DistilGPT-2, version légère de GPT-2.\n",
        "import transformers # Charge le package Transformers.\n",
        "from transformers import AutoModelForCausalLM # API générique pour charger un modèle de langage causal (next-token).\n",
        "from transformers import AutoTokenizer # API générique pour charger le tokenizer compatible.\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map = \"auto\") # Télécharge le modèle pré-entraîné et le place automatiquement sur CPU/GPU(s) selon la RAM VRAM disponible.\n",
        "model.config.use_cache = True # Active le cache de clés/valeurs pour accélérer la génération auto-rgressive.\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True) # Télécharge le tokenizer lié au modèle. trust_remote_code=True permet du code de tokenizer custom si besoin.\n",
        "# Below, we use padding to get all tensors with same length\n",
        "tokenizer.pad_token = tokenizer.eos_token # Définit le token de padding égal au token EOS. GPT-2 n’a pas de token PAD natif, on réutilise EOS (end of sentence).\n",
        "tokenizer.padding_side = 'right' # Pad à droite des séquences. Important pour cohérence forme [tokens, PAD, PAD].\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id # Synchronise l’ID du token de pad avec celui d’EOS.\n",
        "\n",
        "generation_configuration = model.generation_config # Récupère l’objet de configuration de génération attaché au modèle.\n",
        "generation_configuration.pad_token_id = tokenizer.eos_token_id # Assure cohérence entre modèle et tokenizer pour le padding.\n",
        "generation_configuration.eos_token_id = tokenizer.eos_token_id # Spécifie l’ID du token de fin de séquence.\n",
        "generation_configuration.max_new_tokens = 1024 # Limite le nombre de nouveaux tokens générés. Note: le contexte total GPT-2 est 1024, demander 1024 new tokens peut tronquer l’input si long.\n",
        "\n",
        "# Below variables depend on how we sample our model\n",
        "generation_configuration.temperature = 0.7 # Ajuste l’entropie de l’échantillonnage. Plus bas = plus déterministe.\n",
        "generation_configuration.top_p = 0.9 # Nucleus sampling: ne tire que dans le plus petit ensemble de tokens dont la probabilité cumulée ≥ 0.9.\n",
        "generation_configuration.top_k = 20 # Tronque le vocabulaire aux 20 tokens les plus probables avant échantillonnage.\n",
        "generation_configuration.do_sample = True # Enable sampling for generation"
      ],
      "metadata": {
        "id": "fv7QPf06sOK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt): # Déclare une fonction utilitaire.\n",
        "\n",
        "  encoded = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(device) # Tokenise le prompt en IDs, ajoute tokens spéciaux si définis, retourne un tenseur PyTorch et l’envoie sur device.\n",
        "  out = model.generate(input_ids=encoded, repetition_penalty=2.0, do_sample=True) # Lance la génération auto-rressive avec pénalité de répétition (pénalise la ré-utilisation abusive de n-grams) et avec échantillonnage stochastique activé.\n",
        "  string_decoded = tokenizer.decode(out[0].tolist(), clean_up_tokenization_spaces=True) # Convertit les IDs générés en texte brut.\n",
        "  print(string_decoded) # Affiche le texte produit."
      ],
      "metadata": {
        "id": "sneL-0uFygub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('this is') # Exemple d’appel. Produit une complétion."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngah4OAj42TY",
        "outputId": "a42a09f5-ae09-455d-a5ac-4ebf3213e082",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the and that both both both both two as in which\n",
            " ()) if than, to or any oror:. - -- a not know they we of<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('how are you') # Deuxième exemple."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnCermsM45iz",
        "outputId": "d10301c2-3029-4f65-a45a-d21c24efa478",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how are you?›\n",
            "The answer to this question is no, and I think the answers in that article will be interesting. In fact it's not even remotely important for me if they're anything like my own or those of a group who have come out with similar opinions about myself:<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer # Importe la config et l’entraîneur SFT (Supervised Fine-Tuning).\n",
        "from transformers import TrainingArguments # Importe la classe d’arguments HF. Ici on n’utilise que SFTConfig, mais import ok.\n",
        "\n",
        "# set up training arguments\n",
        "training_args = SFTConfig( #Crée une config d’entraînement supervisé.\n",
        "      gradient_accumulation_steps=1, # Pas d’accumulation de gradient. Un update par batch.\n",
        "      num_train_epochs=1, # Une époque sur le split train.\n",
        "      learning_rate=2e-4, # LR initiale. Assez élevée pour un petit modèle.\n",
        "      fp16=True, # Active Half-Precision si GPU le permet. Gagne en vitesse/mémoire.\n",
        "      output_dir=\"logs\", # Dossier de sorties: checkpoints, logs.\n",
        "      lr_scheduler_type=\"cosine\", # Plan de décroissance du LR cosinus.\n",
        "      warmup_ratio=0.05, # 5% d’étapes en warmup pour stabiliser les gradients.\n",
        "      group_by_length=True, # Regroupe par longueur pour réduire padding et accélérer.\n",
        "      max_length=512, # Tronque les entrées à 512 tokens pour l’entraînement.\n",
        "      # neftune_noise_alpha=5, # Add noise to the embeddings (NEFTune) to improve robustness.\n",
        "      do_sample=True, # Explicitly enable sampling in the training config\n",
        ") # Fin de la Config\n",
        "\n",
        "# Fine Tuning Dataset\n",
        "# Learning Rate is critical to Gradient Descent\n",
        "# A dynamic learning rate is best (dynamic alpha)\n",
        "# Towards the end of training -> small learning rate\n",
        "\n",
        "# batch_size cannot be very large -> GPU memory limitation -> do an iteration of training, calculate gradients, but don't update the weights\n",
        "# do another iteration, calculate gradients, add them to the previous iteration's gradients\n",
        "# and then update the weights\n",
        "\n",
        "#initialize the SFTTrainer\n",
        "trainer = SFTTrainer(model=model, train_dataset=training_dataset, processing_class=tokenizer, args=training_args)\n",
        "# Construit un entraîneur SFT:\n",
        "# model: DistilGPT-2 chargé\n",
        "# train_dataset: split train du dataset\n",
        "# processing_class=tokenizer: objet utilisé pour tokeniser et formater les batchs\n",
        "# args: hyperparamètres ci-dessus\n",
        "# Remarque: pour un dataset chat, il faut souvent préciser quelles colonnes contiennent prompt/réponse, ou fournir un template de formatage.\n",
        "# Ici, TRL infère si les colonnes correspondent au schéma attendu du dataset choisi."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIpyWr335Qmn",
        "outputId": "05e1ba87-ee4c-4c49-e947-20fbb6507fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "YhPLxku78B2L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "9183b34f-bfde-4f67-e4bb-8d50056e10d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:45, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.309500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.322100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.655800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.335300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.265300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.435100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.296600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.141700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.298700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.156400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.236300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.385400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=125, training_loss=3.326212585449219, metrics={'train_runtime': 46.4003, 'train_samples_per_second': 21.552, 'train_steps_per_second': 2.694, 'total_flos': 94701699661824.0, 'train_loss': 3.326212585449219, 'entropy': 3.438341569900513, 'num_tokens': 363618.0, 'mean_token_accuracy': 0.37988496422767637, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0c34f7b9",
        "outputId": "3181dcd0-8015-42e4-ee60-a5bef983bd27",
        "collapsed": true
      },
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"your_project_name\", name=\"your_run_name\") # Replace with your desired project and run names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/entropy</td><td>▄▅▆▄▄▄▃▆▂▃▅▅█▅▅▅▃▅▃▄▃▃▃▂▃▁▃▂▂▃▅█▅▅▆▃▅▃▄▆</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▃▅▅▆▆▇▁▂▂▃▃▅▅▆▇█▂▂▃▃▄▅▆▆▇█▂▂▃▃▄▅▆▆▇█</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▃▅▅▆▆▇▁▂▂▃▃▅▅▆▆▇▁▂▂▃▃▅▆▆▇█▂▂▃▃▄▅▆▆▇█</td></tr><tr><td>train/grad_norm</td><td>▃▄█▃▃▂▂▅▂▂▄ ▇▂▂▂▁▃▁▁▁▂▆▂▂▃▂▃▁▂▄ █▂▂▂▁▃▁▃</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▄▃▂▂▁██▇▇▆▄▃▂▂▁██▇▇▆▄▃▂▂▁██▇▇▆▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>▃▅▆▄▅▄▃▇▃▃▅▅█▅▅▅▃▅▄▄▁▃▃▃▃▃▁▄▁▂▅▅█▅▅▅▃▅▄▆</td></tr><tr><td>train/mean_token_accuracy</td><td>▃▄▂▄▅▄▇▃▄▅▃▄▁▂▅▃▅▄▄▅▆▇▆▅▅▅█▅▇▇▄▁▂▅▃▅▄▄▅▄</td></tr><tr><td>train/num_tokens</td><td>▂▂▂▃▃▄▄▄▄▅▁▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇▇██▁▁▂▂▂▃▃▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>94701699661824.0</td></tr><tr><td>train/entropy</td><td>3.43834</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>125</td></tr><tr><td>train/grad_norm</td><td>2.76684</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.3854</td></tr><tr><td>train/mean_token_accuracy</td><td>0.37988</td></tr><tr><td>train/num_tokens</td><td>363618</td></tr><tr><td>train_loss</td><td>3.32621</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">your_run_name</strong> at: <a href='https://wandb.ai/hugo-sirvent-estaca/your_project_name/runs/c2jp993a' target=\"_blank\">https://wandb.ai/hugo-sirvent-estaca/your_project_name/runs/c2jp993a</a><br> View project at: <a href='https://wandb.ai/hugo-sirvent-estaca/your_project_name' target=\"_blank\">https://wandb.ai/hugo-sirvent-estaca/your_project_name</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251025_103405-c2jp993a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251025_110914-rei602qm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hugo-sirvent-estaca/your_project_name/runs/rei602qm' target=\"_blank\">your_run_name</a></strong> to <a href='https://wandb.ai/hugo-sirvent-estaca/your_project_name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hugo-sirvent-estaca/your_project_name' target=\"_blank\">https://wandb.ai/hugo-sirvent-estaca/your_project_name</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hugo-sirvent-estaca/your_project_name/runs/rei602qm' target=\"_blank\">https://wandb.ai/hugo-sirvent-estaca/your_project_name/runs/rei602qm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hugo-sirvent-estaca/your_project_name/runs/rei602qm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7db9aac08350>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('how are you')"
      ],
      "metadata": {
        "id": "YHXhp3Hw8EsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7d3e2b-5856-4970-c788-33147fcad46e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how are you?\n",
            " (). for that<|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}